{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MagNet: Model the Geomagnetic Field Chapter 1\n",
    "## Develop the CNN Model\n",
    "\n",
    "\n",
    "![HELIO_GRAPHIC_URL](https://ngdc.noaa.gov/geomag/img/challenge-banner.png \"HELIO\")\n",
    "\n",
    "* Creator(s): Rob.Redmon@noaa.gov (1,2), Manoj.C.Nair@noaa.gov (2,3), LiYin.Young@noaa.gov (2,3)\n",
    "* Affiliation(s):\n",
    "    * 1. National Centers for Environmental Information ([NCEI](https://www.ncei.noaa.gov/)), National Oceanic and Atmospheric Administration (NOAA),\n",
    "    * 2. NOAA Center for Artificial Intelligence ([NCAI](https://noaa.gov/ai)),\n",
    "    * 3. Cooperative Institute for Research for Environmental Sciences [CIRES](https://cires.colorado.edu/).\n",
    "* History\n",
    "    * 2023-08: Content reorganized for the [NCAI](https://noaa.gov/ai) <i>Learning Journey</i> library. No significant technical changes from previous version.\n",
    "    * 2022-06: Initial notebook version developed for the [TAI4ES 2022 Summer School](https://www2.cisl.ucar.edu/events/tai4es-2022-summer-school).\n",
    "* Acknowledgements:\n",
    "    * Original funding support was provided by the NCEI Innovates program.\n",
    "    * Post-model inference and evaluation were created for the NCAR and [AI2ES](https://www.ai2es.org/) [TAI4ES 2022 Summer School](https://www2.cisl.ucar.edu/events/tai4es-2022-summer-school).\n",
    "    * Feature exploration through model training was inspired by the benchmark [DrivenData blogpost](https://www.drivendata.co/blog/model-geomagnetic-field-benchmark/), developed for NOAA's [MagNet competition](https://ngdc.noaa.gov/geomag/mag-net-challenge.html)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "<b> Chapter 1 \"Develop the CNN Model\"</b> of the two notebook series, provides the benchmark machine learning modeling experience for a key space weather storm indicator, the disturbance-storm-time (<i>Dst</i>) index, for the 2020 NOAA competition, \"MagNet: Model the Geomagnetic Field\".  This notebook is based on the second-place solution to the MagNet competition held by the National Oceanic and Atmospheric Administration (NOAA) and National Aeronautics and Space Administration (NASA) in 2020-21.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prerequisites\n",
    "\n",
    "* <b>Python intermediate proficiency for data science:</b> SciPy, Pandas, NumPy, MatplotLib,\n",
    "* <b>Machine Learning intermediate experience:</b> ML for supervised modeling of time series data using neural networks. We use the Keras framework for TensorFlow in this notebook to create a Convolutional Neural Networks(CNN).\n",
    "* <b>Space Weather introductory knowledge:</b> Basic familiarity of the Solar Wind and the Disturbance Storm Time activity index (<i>Dst</i>). For introductory materials on space weather and its effects on the technological systems we rely on, two resources are:\n",
    "    * [NASA's Space Place](https://spaceplace.nasa.gov/spaceweather/),\n",
    "    * [NOAA's Space Weather Prediction Center (SWPC)](https://www.swpc.noaa.gov/), in particular their community dashboards."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Targeted Level\n",
    "This notebook is targeted towards learners with beginner to intermediate experience in space weather topics, and intermediate experience in modeling time series data with neural networks."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning Outcomes\n",
    "\n",
    "By engaging in this notebook, the learner will:\n",
    "1. Gain experience developing a regression model to predict the space weather disturbance-storm-time (<i>Dst</i>) index.\n",
    "2. Build the functional benchmark CNN model from the NOAA MagNet competition.\n",
    "2. Get familiar with using imperfect solar wind observations, as the modeling features.\n",
    "3. Run the trained model on classical space weather events."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Info: </b>\n",
    "In this notebook, you'll notice color-coded boxes, which provide hints, exercises, and warnings. Here is the color-coding breakdown:\n",
    "</div>\n",
    "\n",
    "* <span style=\"color:blue\">Hint/Tip/Info: </span> Helpful context and guidance, as a blue alert-info box\n",
    "* <span style=\"color:green\">Exercise: </span> Interactive activity / exercise, as a green alert-success box\n",
    "* <span style=\"color:orange\">Be Aware: </span> Caution / Caveat, as a yellow alert-warn box\n",
    "* <span style=\"color:red\">Danger: </span> Conditions under which code may create an error, as a red alert-danger box"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tutorial Material"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Background on Geospace Space Weather\n",
    "\n",
    "Just like the terrestrial weather we are used to experiencing in our daily lives, weather also occurs in the space environment. If you'd like a general primer on space weather and its effects on the technological systems we rely on, check out [NASA's Space Place](https://spaceplace.nasa.gov/spaceweather/), as well as [NOAA's Space Weather Prediction Center (SWPC)](https://www.swpc.noaa.gov/), in particular their community dashboards.\n",
    "\n",
    "![HELIO_GRAPHIC_URL](https://ngdc.noaa.gov/geomag/img/challenge-banner.png \"HELIO\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Background on the Geomagnetic Field\n",
    "\n",
    "The efficient transfer of energy from solar wind into the Earth‚Äôs magnetic field causes geomagnetic storms. The resulting variations in the magnetic field increase errors in magnetic navigation. The disturbance-storm-time index, or <i>Dst</i>, is a measure of the severity of the geomagnetic storm.\n",
    "\n",
    "As a key specification of the magnetospheric dynamics, the <i>Dst</i> index is used to drive geomagnetic disturbance models such as NOAA/NCEI‚Äôs High Definition Geomagnetic Model - Real-Time (HDGM-RT).\n",
    "![HDGMRT_GRAPHIC_URL](https://www.ngdc.noaa.gov/geomag/HDGM/images/HDGM-RT_2003_storm_720p.gif \"HDGM-RT\")\n",
    "\n",
    "In 2020-2021, NOAA and NASA conducted an international crowd sourced data science competition ‚ÄúMagNet: Model the Geomagnetic Field‚Äù:\n",
    "https://www.drivendata.org/competitions/73/noaa-magnetic-forecasting/\n",
    "\n",
    "Empirical models have been proposed as early as in 1975 to forecast <i>Dst</i> solely from solar-wind observations at the Lagrangian (L1) position by satellites such as NOAA‚Äôs Deep Space Climate Observatory (DSCOVR) or NASA's Advanced Composition Explorer (ACE). Over the past three decades, several models were proposed for solar wind forecasting of <i>Dst</i>, including empirical, physics-based, and machine learning approaches. While the ML models generally perform better than models based on the other approaches, there is still room to improve, especially when predicting extreme events. More importantly, we intentionally sought solutions that work on the raw, real-time data streams and are agnostic to sensor malfunctions and noise.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Software\n",
    "This notebook has been tested using the following environments:\n",
    "* Google Colaboratory (Python 3.10.12) with no need to install additional packages.\n",
    "    * CPU, GPU, TPU tested\n",
    "* Anaconda (Python 3.9.16) with the following key package versions:\n",
    "    * Keras TensorFlow 2.8.0\n",
    "    * Pandas 1.5.3\n",
    "    * Matplotlib 3.7.1\n",
    "    * CPU, and GPU tested"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeling Task\n",
    "\n",
    "The MagNet competition task was to develop models for forecasting <i>Dst</i> that push the boundary of predictive performance, under operationally viable constraints, using the real-time solar-wind (RTSW) data feeds from NOAA‚Äôs DSCOVR and NASA‚Äôs ACE satellites. Improved models can provide more advanced warning of geomagnetic storms and reduce errors in magnetic navigation systems. Specifically, given one week of data ending at t minus 1 minute, the model must forecast <i>Dst</i> at time t and t plus one hour.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question: </b> Can you describe the physical process between solar wind and ground geomagnetic disturbances? What is the <i>Dst</i> index primarily used for?\n",
    "Roughly 85% of the time, near Earth is geomagnetically quiet. How might these infrequent solar wind events make modeling their predicted effects challenging? How might you make an accurate model with very few extreme events/samples?\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Notes\n",
    "\n",
    "The target <i>Dst</i> values are measured by 4 ground-based observatories near the equator. These values are then averaged to provide a measurement of <i>Dst</i> for any given hour.\n",
    "To ensure similar distributions between the training and test data, the data is separated into three non-contiguous periods. All data are provided with a `period` and `timedelta` multi-index which indicates the relative timestep for each observation within a period, but not the real timestamp. The period identifiers and timedeltas are common across datasets. Converting back from our index date and time to real geophysical date and time is as simple as adding the start date/time in the table below to the relative timestep provided with the data.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center\">Table: Dataset Period Time Ranges</div>\n",
    "\n",
    "| Period  | Beginning               | End                      |\n",
    "|---------|-------------------------|--------------------------|\n",
    "| train_a | 1998, 2, 16, '00:00:00' | 2001, 5, 31, '23:59:00'  |\n",
    "| train_b | 2013, 6, 1, '00:00:00'  | 2019, 5, 31, '23:59:00'  |\n",
    "| train_c | 2004, 5, 1, '00:00:00'  | 2010, 12, 31, '23:59:00' |\n",
    "|  test_a | 2001, 6, 1, '00:00:00'  | 2004, 4, 30, '23:59:00'  |\n",
    "|  test_b | 2011, 1, 1, '00:00:00'  | 2013, 5, 31, '23:59:00'  |\n",
    "|  test_c | 2019, 6, 1, '00:00:00'  | 2020, 10, 31, '23:59:00' |\n",
    "\n",
    "\n",
    "![Figure_Activity_and_Training_Splits.png](https://github.com/ai2es/tai4es-trustathon-2022/raw/space/space/notebook_figures/Figure_Activity_and_Training_Splits.png)\n",
    "<i>Figure: Plot shows solar activity as the sunspot number (SSN) (red), the geomagnetic disturbance-storm-time (<i>Dst</i>) index (blue), and the data segments. The time range shown is January 1998 through December 2022, roughly corresponding to two solar-cycles. The data for periods ‚Äútrain_a‚Äù, ‚Äútrain_b‚Äù, and ‚Äútrain_c‚Äù  were provided to the participants as ‚Äúpublic‚Äù data. The data for periods ‚Äútest_a‚Äù, ‚Äútest_b‚Äù and ‚Äútest_c‚Äù were held back for ‚Äúprivate‚Äù validation. This figure and the table preceding it have been adapted from Nair et al (manuscript in preparation).</i>\n",
    "\n",
    "The competitors used the training part (‚Äútrain_a‚Äù,‚Äùtrain_b‚Äù and ‚Äútrain_c‚Äù) data to develop and improve their models. When they submitted a model, the competition platform used the test data sets (‚Äútest_a‚Äù,‚Äùtest_b‚Äù and ‚Äútest_c‚Äù)  to calculate the accuracy of the model. The model evaluation was done separately for a public leaderboard and for a private leaderboard. The public leaderboard was openly accessible whereas the private leaderboard was restricted to the competition administrators. The  data from all of the training sets (a, b, and c) were used on the public leaderboard and private leaderboard. We randomly sampled rows to be included in the public and private leaderboard. Based on relative performance from the public leaderboard as a clue, the teams iterated their models. The final ranking of the models was done on the private leaderboard.\n",
    "\n",
    "<b>Input data sources (i.e. features)</b>:\n",
    "\n",
    "* Satellite measurements of the solar wind, including direction, speed, density and temperature, at 1-minute cadence.\n",
    "* Position of the satellite used for solar wind measurements. The ACE and DSCOVR satellites are positioned just outside Earth's exosphere approximately 1% of the distance from Earth to Sun. As noted above, this is referred to as the Sun Earth L1 position.\n",
    "* Number of sunspots on the Sun, measured monthly.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Info:</b> Here is a short description of several of these inputs (features) observed by the ACE or DSCOVR satellites:\n",
    "\n",
    "* bt - Interplanetary-magnetic-field magnitude (nT)\n",
    "* bx_gsm - Interplanetary-magnetic-field X-component in geocentric solar magnetospheric (GSM) coordinate (nT)\n",
    "* by_gsm - Interplanetary-magnetic-field Y-component in GSM coordinate (nT)\n",
    "* bz_gsm - Interplanetary-magnetic-field Z-component in (GSM) coordinate (nT)\n",
    "* density - Solar wind proton density (N/cm^3)\n",
    "* speed - Solar wind bulk speed (km/s) flowing from Sun to Earth\n",
    "* temperature - Solar wind ion temperature (Kelvin)\n",
    "</div>\n",
    "\n",
    "To get a feeling for the GSM coordinate reference frame:\n",
    "\n",
    "The X-axis is oriented from the Earth to the Sun. The positive Z-axis is chosen to be in the same sense as the northern magnetic pole. And the Y-axis is defined to be perpendicular to the Earth's magnetic dipole so that the X-Z plane contains the dipole axis. For additional details, see [here](https://www.spenvis.oma.be/help/background/coortran/coortran.html#GSM).\n",
    "\n",
    "To see how several of these parameters look during an example space weather event see [Figure 5](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2018SW001897#swe20716-fig-0005) of Redmon et al., 2018.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Acquire Data\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Info: </b>\n",
    "The competition discussed in this notebook used <i>public</i> data for development and the public leaderboard. A <i>private</i> dataset was kept internal during the competition for use in scoring by the organizers. Since the competition has passed, both datasets are publicly accessible from NOAA. We will build and evaluate the model using the competition's <i>public</i> data and evaluate storm event case studies using the competition's <i>private</i> data.\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "#download the data\n",
    "if [ ! -d \"data\" ]; then\n",
    "  wget https://ngdc.noaa.gov/geomag/data/geomag/magnet/public.zip\n",
    "  unzip public.zip\n",
    "  wget https://ngdc.noaa.gov/geomag/data/geomag/magnet/private.zip\n",
    "  unzip private.zip\n",
    "  mkdir data\n",
    "  mv public data\n",
    "  mv private data\n",
    "fi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Import  Input (Features) and Output (Labels) as Pandas DataFrames"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# load dataframes from csv files\n",
    "data_folder = \"data\"\n",
    "solar_cols = [\"period\", \"timedelta\", \"bx_gsm\", \"by_gsm\", \"bz_gsm\", \"bt\", \"speed\", \"density\", \"temperature\"]\n",
    "solar_train = pd.read_csv(os.path.join(data_folder, \"public\", \"solar_wind.csv\"), usecols=solar_cols, dtype={\"period\": \"category\"})\n",
    "dst_train = pd.read_csv(os.path.join(data_folder, \"public\", \"dst_labels.csv\"), dtype={\"period\": \"category\"})\n",
    "sunspots_train = pd.read_csv(os.path.join(data_folder, \"public\", \"sunspots.csv\"), dtype={\"period\": \"category\"})\n",
    "solar_test = pd.read_csv(os.path.join(data_folder, \"private\", \"solar_wind.csv\"), usecols=solar_cols, dtype={\"period\": \"category\"})\n",
    "dst_test = pd.read_csv(os.path.join(data_folder, \"private\", \"dst_labels.csv\"), dtype={\"period\": \"category\"})\n",
    "sunspots_test = pd.read_csv(os.path.join(data_folder, \"private\", \"sunspots.csv\"), dtype={\"period\": \"category\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Exploration\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Info:</b> We'll explore our input (feature) and output (label) data to better understand it's data architecture, statistical description and basic input-output relationships.</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot distributions\n",
    "fig, axs = plt.subplots(4, 2, figsize=(20, 12))\n",
    "for i, var in enumerate([\"bx_gsm\", \"by_gsm\", \"bz_gsm\", \"bt\", \"speed\", \"density\", \"temperature\", \"smoothed_ssn\"]):\n",
    "  plt.sca(axs.flat[i])\n",
    "  if var == \"smoothed_ssn\":\n",
    "    sunspots_train[\"smoothed_ssn\"].hist(bins=50)\n",
    "  else:\n",
    "    solar_train[var].hist(bins=50)\n",
    "  axs.flat[i].set_title(var)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature Relationship\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> For merge the pandas Dataframe, users can assign the approach to merge:\n",
    "left, right, outter, inner, cross. The approach is similar to the \"join\" function in SQL. In the following example, \"left\" means to preserve the key order and perform left outer join. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> <b>Correlation matrix:</b>\n",
    "Note that this is a slow command (several minutes) unless you have a GPU or TPU equivalent processor (then it's ~1 min).\n",
    "Take advantage of Pandas DataFrame and merge our Input (Feature) and Output (Label) data.\n",
    "i.e. merge, Solar Wind + Sunspots + Satellite Location + <i>Dst</i></div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "solar_sunspots = solar_train.merge(sunspots_train, how='left')\n",
    "corr = solar_sunspots.merge(dst_train, how='left').fillna(method=\"ffill\").corr()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.matshow(corr, cmap='seismic', vmin=-1, vmax=1, fignum=1)\n",
    "plt.xticks(range(corr.shape[1]), corr.columns, rotation=90)\n",
    "plt.gca().xaxis.tick_bottom()\n",
    "plt.yticks(range(corr.shape[1]), corr.columns)\n",
    "\n",
    "\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title(\"Feature Correlation Heatmap\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "del solar_sunspots\n",
    "del corr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Plot correlation with Dst"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot correlations with dst\n",
    "fig, axs = plt.subplots(4, 2, figsize=(20, 20))\n",
    "for i, var in enumerate([\"bx_gsm\", \"by_gsm\", \"bz_gsm\", \"bt\", \"speed\", \"density\", \"temperature\", \"smoothed_ssn\"]):\n",
    "  if var == \"smoothed_ssn\":\n",
    "    df = dst_train.merge(sunspots_train[[var,\"period\", \"timedelta\"]], 'left', on=[\"period\", \"timedelta\"])\n",
    "  else:\n",
    "    df = dst_train.merge(solar_train[[var,\"period\", \"timedelta\"]], 'left', on=[\"period\", \"timedelta\"])\n",
    "  axs.flat[i].scatter(df[var].values, df['dst'], s=2)\n",
    "  axs.flat[i].set_ylabel('dst')\n",
    "  axs.flat[i].set_xlabel(var)\n",
    "del df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Generation\n",
    "\n",
    "The model uses input data in the\n",
    "$(x, y, z)$ coordinate system, rather than the angular coordinate systems, so that the mean and standard deviation can be easily calculated. We use the GSM coordinate system, rather than GSE, since we found this gives better performance (see https://www.mssl.ucl.ac.uk/grid/iau/extra/local_copy/SP_coords/geo_sys.htm for details on these coordinate systems).\n",
    "\n",
    "Periods where the temperature data is $< 1$ are excluded from the training data, since this is not physically realistic and suggests that sensors are malfunctioning. Missing data is filled\n",
    "by linear interpolation (to reduce noise, the interpolation uses a smoothed rolling average,\n",
    "rather than just the 2 points immediately\n",
    "before and after the missing part).\n",
    "\n",
    "Data is normalized by subtracting the median and dividing by the interquartile range (this approach is used rather\n",
    "than the more usual mean and standard deviation because some variables have asymmetric distributions with\n",
    "long tails).\n",
    "\n",
    "To reduce the volume of data and the training time, data is aggregated  in 10-minute increments, taking the mean and standard deviation of each feature in\n",
    "the increment. This could alternatively be done as the first layer of the neural network, but it's more efficient\n",
    "to do it as a preprocessing step."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-block alert-success\"> <b>Info: </b> Prepare data for training or prediction, by returning 10-minute aggregates. Aggregate solar_data into 10-minute intervals and calculate the mean and standard deviation. Merge with sunspot data. Normalize the training data and save the scaling parameters in a dataframe (these are needed to transform data for prediction). </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> <b>Note: </b>This method modifies the input dataframes solar, sunspots, and dst. If you want to keep the original dataframes unaltered, pass copies, e.g. prepare_data_1_min(solar.copy(), sunspots.copy(), dst.copy()). </div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function to preprocess data; we will use this to prepare the data for training\n",
    "# and validation\n",
    "\n",
    "def prepare_data_1_min(\n",
    "    solar: pd.DataFrame,\n",
    "    sunspots: Union[pd.DataFrame, float],\n",
    "    dst: pd.DataFrame = None,\n",
    "    norm_df=None,\n",
    "    output_folder: str = None,\n",
    "    coord_system: str = \"gsm\",\n",
    "    output_freq: str = \"10_minute\",\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    If ``dst`` is ``None``, prepare dataframe of feature variables only for prediction\n",
    "    using previously-calculated normalization scaling factors in ``norm_df``.\n",
    "    If ``dst`` is not ``None``, prepare dataframe of feature variables and labels for\n",
    "    model training. Calculate normalization scaling factors and\n",
    "    save in ``output_folder``. In this case ``output_folder`` must not be ``None``.\n",
    "\n",
    "    Args:\n",
    "        solar: DataFrame containing solar wind data. This function uses the GSM\n",
    "            co-ordinates.\n",
    "        sunspots: DataFrame containing sunspots data, or float. If dataframe, will be\n",
    "            merged with solar data using timestamp. If float, all rows of output data\n",
    "            will use this number.\n",
    "        dst: ``None``, or DataFrame containing the disturbance storm time (Dst) data,\n",
    "        i.e. the labels for training\n",
    "        norm_df: ``None``, or DataFrame containing the normalization scaling factors to\n",
    "            apply\n",
    "        output_folder: Path to the directory where normalization dataframe will be saved\n",
    "        coord_system: either \"gsm\" or \"gse\"\n",
    "        output_freq: \"10_minute\" or \"hour\", how to aggregate the output data\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        solar: DataFrame containing processed data and labels\n",
    "        train_cols: list of training columns\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "      os.mkdir(output_folder)\n",
    "\n",
    "    # convert timedelta\n",
    "    solar[\"timedelta\"] = pd.to_timedelta(solar[\"timedelta\"])\n",
    "\n",
    "    # merge data\n",
    "    solar[\"days\"] = solar[\"timedelta\"].dt.days\n",
    "    if isinstance(sunspots, pd.DataFrame):\n",
    "        sunspots[\"timedelta\"] = pd.to_timedelta(sunspots[\"timedelta\"])\n",
    "        sunspots.sort_values([\"period\", \"timedelta\"], inplace=True)\n",
    "        sunspots[\"month\"] = list(range(len(sunspots)))\n",
    "        sunspots[\"month\"] = sunspots[\"month\"].astype(int)\n",
    "        sunspots[\"days\"] = sunspots[\"timedelta\"].dt.days\n",
    "        solar = pd.merge(\n",
    "            solar,\n",
    "            sunspots[[\"period\", \"days\", \"smoothed_ssn\", \"month\"]],\n",
    "            \"left\",\n",
    "            [\"period\", \"days\"],\n",
    "        )\n",
    "    else:\n",
    "        solar[\"smoothed_ssn\"] = sunspots\n",
    "    solar.drop(columns=\"days\", inplace=True)\n",
    "    if dst is not None:\n",
    "        dst[\"timedelta\"] = pd.to_timedelta(dst[\"timedelta\"])\n",
    "        solar = pd.merge(solar, dst, \"left\", [\"period\", \"timedelta\"])\n",
    "    solar.sort_values([\"period\", \"timedelta\"], inplace=True)\n",
    "    solar.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # remove anomalous data (exclude from training and fill for prediction)\n",
    "    solar[\"bad_data\"] = False\n",
    "    solar.loc[solar[\"temperature\"] < 1, \"bad_data\"] = True\n",
    "    solar.loc[solar[\"temperature\"] < 1, [\"temperature\", \"speed\", \"density\"]] = np.nan\n",
    "    for p in solar[\"period\"].unique():\n",
    "        curr_period = solar[\"period\"] == p\n",
    "        solar.loc[curr_period, \"train_exclude\"] = (\n",
    "            solar.loc[curr_period, \"bad_data\"].rolling(60 * 24 * 7, center=False).max()\n",
    "        )\n",
    "\n",
    "    # fill missing data\n",
    "    if \"month\" in solar.columns:\n",
    "        solar[\"month\"] = solar[\"month\"].fillna(method=\"ffill\")\n",
    "    if coord_system == \"gsm\":\n",
    "        train_cols = [\n",
    "            \"bt\",\n",
    "            \"density\",\n",
    "            \"speed\",\n",
    "            \"bx_gsm\",\n",
    "            \"by_gsm\",\n",
    "            \"bz_gsm\",\n",
    "            \"smoothed_ssn\",\n",
    "        ]\n",
    "    elif coord_system == \"gse\":\n",
    "        train_cols = [\n",
    "            \"bt\",\n",
    "            \"density\",\n",
    "            \"speed\",\n",
    "            \"bx_gse\",\n",
    "            \"by_gse\",\n",
    "            \"bz_gse\",\n",
    "            \"smoothed_ssn\",\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid coord system {coord_system}\")\n",
    "    train_short = [c for c in train_cols if c != \"smoothed_ssn\"]\n",
    "    for p in solar[\"period\"].unique():\n",
    "        curr_period = solar[\"period\"] == p\n",
    "        solar.loc[curr_period, \"smoothed_ssn\"] = (\n",
    "            solar.loc[curr_period, \"smoothed_ssn\"]\n",
    "            .fillna(method=\"ffill\", axis=0)\n",
    "            .fillna(method=\"bfill\", axis=0)\n",
    "        )\n",
    "        # fill short gaps with interpolation\n",
    "        roll = (\n",
    "            solar[train_short]\n",
    "            .rolling(window=20, min_periods=5)\n",
    "            .mean()\n",
    "            .interpolate(\"linear\", axis=0, limit=60)\n",
    "        )\n",
    "        solar.loc[curr_period, train_short] = solar.loc[\n",
    "            curr_period, train_short\n",
    "        ].fillna(roll)\n",
    "        solar.loc[curr_period, train_short] = solar.loc[\n",
    "            curr_period, train_short\n",
    "        ].fillna(solar.loc[curr_period, train_short].mean(), axis=0)\n",
    "\n",
    "    # normalize data using median and inter-quartile range\n",
    "    if norm_df is None:\n",
    "        norm_df = solar[train_cols].median().to_frame(\"median\")\n",
    "        norm_df[\"lq\"] = solar[train_cols].quantile(0.25)\n",
    "        norm_df[\"uq\"] = solar[train_cols].quantile(0.75)\n",
    "        norm_df[\"iqr\"] = norm_df[\"uq\"] - norm_df[\"lq\"]\n",
    "    if output_folder is not None:\n",
    "        norm_df.to_csv(os.path.join(output_folder, \"norm_df.csv\"))\n",
    "    solar[train_cols] = (solar[train_cols] - norm_df[\"median\"]) / norm_df[\"iqr\"]\n",
    "\n",
    "    if dst is not None:\n",
    "        # interpolate target and shift target since we only have data up to t - 1 minute\n",
    "        solar[\"target\"] = (\n",
    "            solar[\"dst\"].shift(-1).interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "        )\n",
    "        # shift target for training t + 1 hour model\n",
    "        solar[\"target_shift\"] = solar[\"target\"].shift(-60)\n",
    "        solar[\"target_shift\"] = solar[\"target_shift\"].fillna(method=\"ffill\")\n",
    "        assert solar[train_cols + [\"target\", \"target_shift\"]].isnull().sum().sum() == 0\n",
    "\n",
    "    # aggregate features\n",
    "    if output_freq == \"10_minute\":\n",
    "        win = 10\n",
    "    elif output_freq == \"hour\":\n",
    "        win = 60\n",
    "    else:\n",
    "        raise ValueError(\"output_freq must be 10_minute or hour.\")\n",
    "    new_cols = [c + suffix for suffix in [\"_mean\", \"_std\"] for c in train_short]\n",
    "    train_cols = new_cols + [\"smoothed_ssn\"]\n",
    "    new_df = pd.DataFrame(index=solar.index, columns=new_cols)\n",
    "    for p in solar[\"period\"].unique():\n",
    "        curr_period = solar[\"period\"] == p\n",
    "        new_df.loc[curr_period] = (\n",
    "            solar.loc[curr_period, train_short]\n",
    "            .rolling(window=win, min_periods=1, center=False)\n",
    "            .agg([\"mean\", \"std\"])\n",
    "            .values\n",
    "        )\n",
    "        new_df.loc[curr_period] = (\n",
    "            new_df.loc[curr_period].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "        )\n",
    "    solar = pd.concat([solar, new_df], axis=1)\n",
    "    solar[train_cols] = solar[train_cols].astype(float)\n",
    "\n",
    "    # sample at output frequency\n",
    "    solar = solar.loc[solar[\"timedelta\"].dt.seconds % (win * 60) == 0].reset_index()\n",
    "\n",
    "    return solar, train_cols"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data generator\n",
    "\n",
    "We make a data generator to generate batches of input data and labels to feed to the keras model. Generating the data dynamically this way saves memory by avoiding the need to assemble all the data in a large array before training. (This large array would consume much more memory than the original data, because there is a lot of overlap in input data between subsequent prediction times, and thus a lot of repetition of the input data). Data is shuffled at the end of each training epoch, so each epoch's batches are different.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">The following data generator dynamically generates batches of training data for the keras model. Each batch consists of multiple time series sequences. See the keras documentation for more details:\n",
    "https://keras.io/getting_started/faq/#what-do-sample-batch-and-epoch-mean\n",
    "</div>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define data generator\n",
    "\n",
    "class DataGen(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        valid_inds: np.ndarray,\n",
    "        y: Optional[np.ndarray] = None,\n",
    "        batch_size: int = 32,\n",
    "        length: int = 24 * 6 * 7,\n",
    "        shuffle: bool = True,\n",
    "    ):\n",
    "        \"\"\"Construct the data generator.\n",
    "\n",
    "        If ``y`` is not ``None``, will generate batches of pairs of x and y data,\n",
    "        suitable for training. If ``y`` is ``None``, will generate batches of ``x``\n",
    "        data only, suitable for prediction.\n",
    "\n",
    "        ``x`` and ``y`` data must already be ordered by period and time. The training\n",
    "        sample generated for an index ``i`` in ``valid_ind`` will have target ``y[i]``\n",
    "        and ``x`` variables from rows ``(i - length + 1)`` to ``i`` (inclusive) of\n",
    "        ``x``.  If there are multiple periods, there should be at least ``(length - 1)``\n",
    "        data points before the first ``valid_ind`` in each period, otherwise the\n",
    "        sequence for that valid_ind will include data from a previous period.\n",
    "\n",
    "        Args:\n",
    "            x: Array containing the x variables ordered by period and time\n",
    "            y: ``None`` or array containing the targets corresponding to the ``x``\n",
    "                variables\n",
    "            batch_size: Size of training batches\n",
    "            valid_inds: Array of ``int`` containing the indices which are valid\n",
    "                end-points of training sequences (for example, we may set ``valid_inds``\n",
    "                so it contains only the data points at the start of each hour).\n",
    "            length: Number of data points in each sequence of the batch\n",
    "            shuffle: Whether to shuffle ``valid_ind`` before training and after\n",
    "                each epoch. For training, it is recommended to set this to ``True``, so\n",
    "                each batch contains a varied sample of data from different times. For\n",
    "                prediction, it should be set to ``False``, so that the predicted values\n",
    "                are in the same order as the input data.\n",
    "        \"\"\"\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.length = length\n",
    "        self.batch_size = batch_size\n",
    "        self.valid_inds = np.copy(valid_inds)\n",
    "        self.shuffle = shuffle\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.valid_inds)\n",
    "\n",
    "    def __get_y__(self):\n",
    "        \"\"\"Return the array of labels indexed by ``valid_ind``.\"\"\"\n",
    "        if self.y is None:\n",
    "            raise RuntimeError(\"Generator has no y data.\")\n",
    "        else:\n",
    "            return self.y[self.valid_inds]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of batches in each epoch.\"\"\"\n",
    "        return int(np.ceil(len(self.valid_inds) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Generate a batch. ``idx`` is the index of the batch in the training epoch.\"\"\"\n",
    "        if (idx < self.__len__() - 1) or (len(self.valid_inds) % self.batch_size == 0):\n",
    "            num_samples = self.batch_size\n",
    "        else:\n",
    "            num_samples = len(self.valid_inds) % self.batch_size\n",
    "        x = np.empty((num_samples, self.length, self.x.shape[1]))\n",
    "        end_indexes = self.valid_inds[\n",
    "            idx * self.batch_size : (idx + 1) * self.batch_size\n",
    "        ]\n",
    "        for n, i in enumerate(end_indexes):\n",
    "            x[n] = self.x[i - self.length : i, :]\n",
    "        if self.y is None:\n",
    "            return x\n",
    "        else:\n",
    "            y = self.y[end_indexes]\n",
    "            return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Code to run at the end of each training epoch.\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.valid_inds)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define and Build our CNN Model\n",
    "\n",
    "#### Define CNN Model\n",
    "The model consists of a set of convolutional layers which detect patterns at progressively longer time spans. At each convolutional\n",
    "layer (except the last), a convolutional filter is applied having size 6 with stride 3, which reduces the size of the output data\n",
    "relative to the input. (The last convolution has small input size, so it just convolves all its 9 inputs\n",
    "together.) The earlier layers recognize low-level features on short time-spans, and these are outputs aggregated into higher-level\n",
    "patterns spanning longer time ranges in the later layers. Cropping is applied at each layer which removes a few data points at the beginning at the\n",
    "sequence to ensure the result will be exactly divisible by 6, so that the last application of the convolutional filter will\n",
    "capture the data at the very end of the sequence. Following all the convolutional\n",
    "layers is a layer which concatenates the last data point of each of the convolution outputs. This concatenation is then fed into a dense layer. The idea of taking the last data point of each convolution\n",
    "is that it represents the patterns  at different timespans leading up to the prediction time: for example, the last data point\n",
    "of the first layer gives the features of the hour before the prediction time, then the second layer gives\n",
    "the last 6 hours, etc.\n",
    "\n",
    "The architecture is somewhat similar to a widely used architecture for image segmentation, the U-Net introduced by Ronneberger, Fischer, and Brox (https://arxiv.org/abs/1505.04597). The U-Net consists of a \"contracting path\", a series of convolutional layers which condense the image, followed by an \"expansive path\" of up-convolution layers which expand the outputs back to the scale of the original image. Combining small-scale and large-scale features allows the network to make localized predictions that also take account of larger surrounding patterns.\n",
    "\n",
    "The idea is also similar to the Temporal Convolutional Network described by Bai, Kolter, and Koltun (https://arxiv.org/abs/1803.01271); however their architecture uses residual (i.e. additive) connections to blend the low-level and high-level features, rather than concatenations.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define the structure of the neural network for 1-minute data\n",
    "\n",
    "def define_model_cnn_1_min() -> Tuple[\n",
    "    tf.keras.Model, List[np.ndarray], int, float, int\n",
    "]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        model: keras model\n",
    "        initial_weights: Array of initial weights used to reset the model to its\n",
    "            original state\n",
    "        epochs: Number of epochs\n",
    "        lr: Learning rate\n",
    "        bs: Batch size\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tf.keras.layers.Input((6 * 24 * 7, 13))\n",
    "    conv1 = tf.keras.layers.Conv1D(50, kernel_size=6, strides=3, activation=\"relu\")(\n",
    "        inputs\n",
    "    )\n",
    "    trim1 = tf.keras.layers.Cropping1D((5, 0))(\n",
    "        conv1\n",
    "    )  # crop from left so resulting shape is divisible by 6\n",
    "    conv2 = tf.keras.layers.Conv1D(50, kernel_size=6, strides=3, activation=\"relu\")(\n",
    "        trim1\n",
    "    )\n",
    "    trim2 = tf.keras.layers.Cropping1D((1, 0))(conv2)\n",
    "    conv3 = tf.keras.layers.Conv1D(30, kernel_size=6, strides=3, activation=\"relu\")(\n",
    "        trim2\n",
    "    )\n",
    "    trim3 = tf.keras.layers.Cropping1D((5, 0))(conv3)\n",
    "    conv4 = tf.keras.layers.Conv1D(30, kernel_size=6, strides=3, activation=\"relu\")(\n",
    "        trim3\n",
    "    )\n",
    "    conv5 = tf.keras.layers.Conv1D(30, kernel_size=9, strides=9, activation=\"relu\")(\n",
    "        conv4\n",
    "    )\n",
    "    # extract last data point of previous convolutional layers (left-crop all but one)\n",
    "    comb1 = tf.keras.layers.Concatenate(axis=2)(\n",
    "        [\n",
    "            conv5,\n",
    "            tf.keras.layers.Cropping1D((334, 0))(conv1),\n",
    "            tf.keras.layers.Cropping1D((108, 0))(conv2),\n",
    "            tf.keras.layers.Cropping1D((34, 0))(conv3),\n",
    "            tf.keras.layers.Cropping1D((8, 0))(conv4),\n",
    "        ]\n",
    "    )\n",
    "    dense = tf.keras.layers.Dense(50, activation=\"relu\")(comb1)\n",
    "    output = tf.keras.layers.Flatten()(tf.keras.layers.Dense(1)(dense))\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "    initial_weights = model.get_weights()\n",
    "    epochs = 3\n",
    "    lr = 0.00025\n",
    "    bs = 32\n",
    "    return model, initial_weights, epochs, lr, bs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ensembling\n",
    "The final model is an ensemble of 5 models with the same structure, trained on different subsets of the data (these are often called \"cross-validation folds\"). This is a common technique in machine learning. The idea is that each model only imperfectly captures the \"true\" relationship between the input and output variables, and partly fits to noise in the training data. But if we average several models, the random noise components will approximately cancel each other out, leaving a more accurate prediction of the true relationship.\n",
    "\n",
    "The training data is segmented by month, and for each model 20% of months are excluded. The data is split by months rather than hours because successive hours are likely to be correlated, meaning test and train sets would be more similar, reducing the benefit of the ensemble. Separate models are trained for times  ùë°  and  ùë°+1 , yielding 10 models in total. To ensure each fold contains a mixture of extreme and moderate Dst periods, we order the months by average Dst, and then leave out every fifth month for each fold (e.g. in fold 0 we exclude months 4, 9, 14..., then in fold 1 we exclude months 1, 10, 15, ...).\n",
    "\n",
    "For reproducibility, we set the numpy, keras, and python random seeds. Results may still not reproduce exactly due to hardware differences."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Model\n",
    "Here we execute the data preparation and training functions we defined earlier."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# preprocess input data\n",
    "output_folder = \"trained_models_cnn\"\n",
    "solar_1_min, train_cols = prepare_data_1_min(\n",
    "    solar_train, sunspots_train, dst_train, output_folder=output_folder, norm_df=None\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train model\n",
    "model_def_1_min = define_model_cnn_1_min\n",
    "model, initial_weights, epochs, lr, bs = model_def_1_min()\n",
    "train_on_prepared_data(\n",
    "        solar_1_min,\n",
    "        model,\n",
    "        initial_weights,\n",
    "        epochs,\n",
    "        lr,\n",
    "        bs,\n",
    "        train_cols,\n",
    "        5,\n",
    "        output_folder,\n",
    "        \"minute\",\n",
    "        False\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate Trained Model\n",
    "\n",
    "#### define function to load saved models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_models(\n",
    "    input_folder: str, num_models: int,\n",
    ") -> Tuple[List[tf.keras.Model], List[tf.keras.Model], pd.DataFrame]:\n",
    "    \"\"\"Define the model structure and load the saved weights of the trained models.\n",
    "\n",
    "    Args:\n",
    "        input_folder: Path to location where model weights are saved\n",
    "        num_models: Number of models trained for each of ``t`` and ``t + 1`` (total\n",
    "            number of models in folder should be ``2 * num_models``)\n",
    "\n",
    "    Returns:\n",
    "        model_t_arr: List of models for time ``t``\n",
    "        model_t_plus_one_arr: List of models for time ``t + 1``\n",
    "        norm_df: DataFrame of scaling factors to normalize the data\n",
    "    \"\"\"\n",
    "    model_t_arr = []\n",
    "    model_t_plus_one_arr = []\n",
    "    for i in range(num_models):\n",
    "        model = tf.keras.models.load_model(\n",
    "            os.path.join(input_folder, \"model_t_{}.h5\".format(i))\n",
    "        )\n",
    "        model_t_arr.append(model)\n",
    "        model = tf.keras.models.load_model(\n",
    "            os.path.join(input_folder, \"model_t_plus_one_{}.h5\".format(i))\n",
    "        )\n",
    "        model_t_plus_one_arr.append(model)\n",
    "    norm_df = pd.read_csv(os.path.join(input_folder, \"norm_df.csv\"), index_col=0)\n",
    "    return model_t_arr, model_t_plus_one_arr, norm_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_folder = \"trained_models_cnn\"\n",
    "model_t_arr, model_t_plus_1_arr, norm_df = load_models(output_folder, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Measure overall performance\n",
    "\n",
    "We score the model on the private dataset, using the root mean squared error metric used in the competition. In the competition, the prediction function was called repeatedly on one week of input data at at time. Here we can speed up the prediction by using the data generator we defined above.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Make predictions for multiple times, which is faster than predicting one at a time.\n",
    "Input data must be sorted by period and time and have a 1-minute frequency, and there must be at least 1 week of data before the first prediction time. </div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Calculate the RMSE for ensembling model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define prediction function\n",
    "\n",
    "def predict_batch(\n",
    "    solar: pd.DataFrame,\n",
    "    sunspots: pd.DataFrame,\n",
    "    prediction_times: pd.DataFrame,\n",
    "    model_t_arr: List[tf.keras.Model],\n",
    "    model_t_plus_one_arr: List[tf.keras.Model],\n",
    "    norm_df: pd.DataFrame,\n",
    "    frequency: str,\n",
    "    output_folder: str,\n",
    "    comb_model: bool = False\n",
    ") -> Tuple[pd.DataFrame, List, List]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        solar: DataFrame containing solar wind data\n",
    "        sunspots: DataFrame containing sunspots data\n",
    "        prediction_times: DataFrame with a single column `timedelta` for which to make\n",
    "            predictions. For each value ``t``, return predictions for ``t`` and\n",
    "            ``t`` plus one hour.\n",
    "        model_t_arr: List of models for time ``t``\n",
    "        model_t_plus_one_arr: List of models for time ``(t + 1)``\n",
    "        norm_df: Scaling factors to normalize the data\n",
    "        frequency: frequency of the model, \"minute\", \"hour\" or \"hybrid\"\n",
    "        output_folder: Path to the directory where normalisation dataframe will be saved\n",
    "        comb_model: if ``True`` assumes a single model that predicts ``t`` and ``t+1``\n",
    "\n",
    "    Returns:\n",
    "        predictions: DataFrame with columns ``timedelta``, ``period``, ``prediction_t``\n",
    "            and ``prediction_t_plus_1``\n",
    "        t0_predictions_set: Dataframe containing the predicted t0 results from five individual models\n",
    "        t1_predictions_set: Dataframe containing the predicted t1 results from five individual models\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # validate input data\n",
    "    solar[\"timedelta\"] = pd.to_timedelta(solar[\"timedelta\"])\n",
    "    diff = solar[\"timedelta\"].diff()\n",
    "    diff.loc[solar[\"period\"] != solar[\"period\"].shift()] = np.nan\n",
    "    valid_diff = solar[\"period\"] == solar[\"period\"].shift(1)\n",
    "    if (frequency in [\"minute\", \"hybrid\"]) and np.any(\n",
    "        diff.loc[valid_diff] != dt.timedelta(minutes=1)\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Input data must be sorted by period and time and have 1-minute frequency.\"\n",
    "        )\n",
    "    elif (frequency == \"hour\") and np.any(\n",
    "        diff.loc[valid_diff] != dt.timedelta(hours=1)\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Input data must be sorted by period and time and have 1-hour frequency.\"\n",
    "        )\n",
    "\n",
    "    # add column to solar to indicate which times we must predict\n",
    "    prediction_times[\"prediction_time\"] = True\n",
    "    solar = pd.merge(solar, prediction_times, on=[\"period\", \"timedelta\"], how=\"left\")\n",
    "    solar[\"prediction_time\"] = solar[\"prediction_time\"].fillna(False)\n",
    "    solar.sort_values([\"period\", \"timedelta\"], inplace=True)\n",
    "    solar.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # prepare data\n",
    "    if frequency == \"minute\":\n",
    "        solar, train_cols = prepare_data_1_min(\n",
    "            solar.copy(), sunspots.copy(), output_folder=output_folder, norm_df=norm_df\n",
    "        )\n",
    "    elif frequency == \"hour\":\n",
    "        solar, train_cols = prepare_data_hourly(\n",
    "            solar.copy(), sunspots.copy(), norm_df=norm_df\n",
    "        )\n",
    "    elif frequency == \"hybrid\":\n",
    "        solar, train_cols = prepare_data_hybrid(\n",
    "            solar.copy(), None, sunspots.copy(), norm_df=norm_df, freq_for_1_min_data=\"10_minute\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid frequency {frequency}\")\n",
    "\n",
    "    # check there is 1 week of data before each valid time\n",
    "    min_data_by_period = solar.groupby(\"period\")[\"timedelta\"].min().to_frame(\"min_time\")\n",
    "    min_data_by_period[\"min_prediction_time\"] = (\n",
    "        solar.loc[solar[\"prediction_time\"]].groupby(\"period\")[\"timedelta\"].min()\n",
    "    )\n",
    "    min_data_by_period[\"data_before_first_prediction\"] = (\n",
    "        min_data_by_period[\"min_prediction_time\"] - min_data_by_period[\"min_time\"]\n",
    "    )\n",
    "    if min_data_by_period[\"data_before_first_prediction\"].min() < dt.timedelta(days=7):\n",
    "        raise RuntimeError(\n",
    "            \"There must be at least 1 week of data before the first prediction time in each period.\"\n",
    "        )\n",
    "\n",
    "    # valid_ind will be the endpoints of the sequences generated by the data generator;\n",
    "    # these must be 1 minute/hour before the prediction time\n",
    "    solar[\"valid_ind\"] = solar[\"prediction_time\"].fillna(False)\n",
    "\n",
    "    # make prediction\n",
    "    predictions = pd.DataFrame(prediction_times[[\"timedelta\", \"period\"]].copy())\n",
    "    valid_ind = solar.loc[solar[\"valid_ind\"]].index.values\n",
    "    sequence_length = 24 * 6 * 7 if frequency in [\"minute\", \"hybrid\"] else 24 * 7\n",
    "    datagen = DataGen(\n",
    "        solar[train_cols].values,\n",
    "        valid_ind,\n",
    "        y=None,\n",
    "        batch_size=100,\n",
    "        length=sequence_length,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    #Create the array for analyzing five models later.\n",
    "    t0_predictions_set = []\n",
    "    t1_predictions_set = []\n",
    "\n",
    "    predictions[\"prediction_t\"] = 0\n",
    "    predictions[\"prediction_t_plus_1\"] = 0\n",
    "    if comb_model:\n",
    "        for m in model_t_arr:\n",
    "            predictions[[\"prediction_t\", \"prediction_t_plus_1\"]] = np.array(m.predict(datagen))\n",
    "    else:\n",
    "        for m in model_t_arr:\n",
    "            curr_model = np.array(m.predict(datagen)).flatten()\n",
    "            t0_predictions_set.append(curr_model)\n",
    "            predictions[\"prediction_t\"] += curr_model\n",
    "        predictions[\"prediction_t\"] /= len(model_t_arr)\n",
    "\n",
    "        for m in model_t_plus_one_arr:\n",
    "            curr_model = np.array(m.predict(datagen)).flatten()\n",
    "            t0_predictions_set.append(curr_model)\n",
    "            predictions[\"prediction_t_plus_1\"] += curr_model\n",
    "        predictions[\"prediction_t_plus_1\"] /= len(model_t_plus_one_arr)\n",
    "\n",
    "    # restrict to allowed range\n",
    "    predictions[\"prediction_t\"] = np.maximum(\n",
    "        -2000, np.minimum(500, predictions[\"prediction_t\"])\n",
    "    )\n",
    "    predictions[\"prediction_t_plus_1\"] = np.maximum(\n",
    "        -2000, np.minimum(500, predictions[\"prediction_t_plus_1\"])\n",
    "    )\n",
    "\n",
    "    return predictions, t0_predictions_set, t1_predictions_set\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Calibrate time column"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dst_test[\"timedelta\"] = pd.to_timedelta(dst_test[\"timedelta\"])\n",
    "# exclude times in the first week + 1 hour of dst_test\n",
    "dst_test = dst_test.loc[dst_test[\"timedelta\"] >= dt.timedelta(days=7, hours=1)].copy()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Measure RMSE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# measure RMSE\n",
    "predictions, t0_predictions_set, t1_predictions_set = predict_batch(\n",
    "    solar_test.copy(), sunspots_test, dst_test, model_t_arr, model_t_plus_1_arr, norm_df, \"minute\", output_folder\n",
    ")\n",
    "\n",
    "#merge true data with prediction results\n",
    "dst_test_1_min = pd.merge(dst_test, predictions, \"left\", [\"timedelta\", \"period\"])\n",
    "dst_test_1_min[\"dst_t_plus_1\"] = dst_test_1_min.groupby(\"period\")[\"dst\"].shift(-1)\n",
    "\n",
    "aveModel_rmse_t = np.sqrt(\n",
    "    mean_squared_error(dst_test_1_min[\"dst\"].values, dst_test_1_min[\"prediction_t\"].values)\n",
    ")\n",
    "\n",
    "valid_ind = dst_test_1_min[\"dst_t_plus_1\"].notnull()\n",
    "aveModel_rmse_t_plus_1 = np.sqrt(\n",
    "    mean_squared_error(\n",
    "        dst_test_1_min.loc[valid_ind, \"dst_t_plus_1\"].values,\n",
    "        dst_test_1_min.loc[valid_ind, \"prediction_t_plus_1\"].values,\n",
    "    )\n",
    ")\n",
    "print(f\"RMSE for time t: {aveModel_rmse_t:0.2f}\")\n",
    "print(f\"RMSE for time t+1: {aveModel_rmse_t_plus_1:0.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next steps\n",
    "\n",
    "Congratulations on engaging with the learning objectives of this MagNet Chapter 1 CNN focused notebook--the benchmark from the NOAA MagNet competition. There is one additional <b>Chapter 2</b> notebook in the MagNet CNN series, on explainable AI (XAI) and includes using the model you've built to explore <i>Dst</i> prediction for user chosen storms.\n",
    "\n",
    "There is an additional NCAI notebook in preparation for this MagNet series:\n",
    "A LSTM notebook from Rob.Redmon.\n",
    "As mentioned in an earlier section, this notebook's precursor is the [TAI4ES Space Weather LSTM Notebook](https://github.com/ai2es/tai4es-trustathon-2022/tree/main/space)\n",
    "\n",
    "Additionally, a web search will provide other <i>Dst</i> modeling notebooks and publications using ML techniques."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examples in the community\n",
    "\n",
    "For a comprehensive treatment of the need to build robust predictions of the <i>Dst</i> space weather storm indicator (e.g. for magnetic navigation applications), see Nair et al., 2023 and references therein:\n",
    "* Nair et al., 2023 (<i>in press</i>) (TODO: Update with public URL as soon as available),\n",
    "\n",
    "For a summary, see:\n",
    "* https://www.drivendata.org/competitions/73/noaa-magnetic-forecasting/\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data statement\n",
    "The competition discussed above used <i>public</i> data for development and the public leaderboard. A <i>private</i> dataset was kept internal during the competition for use in scoring by the organizers. Since the competition has passed, both datasets are publicly accessible from NOAA.\n",
    "\n",
    "All data used in this notebook are publicly available here:\n",
    "* https://ngdc.noaa.gov/geomag/data/geomag/magnet/public.zip\n",
    "* https://ngdc.noaa.gov/geomag/data/geomag/magnet/private.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "* Nair, M., Redmon, R.J., Young, L.Y., Chulliat, A., Trotta, B., Chung, C., Lipstein, G., Slavitt, I. (2023),\"MagNet - a data-science competition to predict Disturbance Storm-time index (Dst) from solar wind data\", Space Weather, <i>In Press</i>. (TODO: update with URL once available.)\n",
    "* [CIRES GeoMag MagNet repository](https://github.com/liyo6397/MagNet/), TODO: update URL to new CIRES repo.\n",
    "* [Trustworthy Artificial Intelligence for Environmental Science 2022 Summer School](https://www2.cisl.ucar.edu/events/tai4es-2022-summer-school), TAI4ES, accessed July 2022.\n",
    "* [TAI4ES Space Weather Notebooks (LSTM, CNN)](https://github.com/ai2es/tai4es-trustathon-2022/tree/main/space), GitHub, accessed July 2022.\n",
    "* [MagNet: Model the Geomagnetic Field](https://ngdc.noaa.gov/geomag/mag-net-challenge.html), NOAA, accessed March 2022.\n",
    "* Chung, C. (2020), \"HOW TO PREDICT DISTURBANCES IN THE GEOMAGENTIC FIELD WITH LSTMS - BENCHMARK\", Blogpost, Accessed March 2022, Available Online: https://drivendata.co/blog/model-geomagnetic-field-benchmark/.\n",
    "* DrivenData (2020), \"MagNet: Model the Geomagnetic Field\", Web Resource, Accessed March 2022, Available Online: https://www.drivendata.org/competitions/73/noaa-magnetic-forecasting/.\n",
    "* [Interpretable Machine Learning by Christop Molnar](https://christophm.github.io/interpretable-ml-book/shap.html)\n",
    "* Redmon, R. J., Seaton, D. B., Steenburgh, R., He, J., & Rodriguez, J. V. (2018). September 2017's geoeffective space weather and impacts to Caribbean radio communications during hurricane response. Space Weather, 16, 1190‚Äì1201. https://doi.org/10.1029/2018SW001897"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metadata\n",
    " * Language / package(s):\n",
    "     * Language: Python,\n",
    "     * Packages: Keras Tensor Flow, Matplotlib, Numpy, Pandas, Scikit-learn\n",
    " * Scientific domain:\n",
    "     * Space Weather, Geomagnetic modeling\n",
    " * Application keywords\n",
    "     * Magnetic Navigation\n",
    " * Geophysical keywords\n",
    "     * Disturbance Storm Index (<i>Dst</i>), Solar Wind\n",
    " * AI keywords\n",
    "     * Convolutional Neural Networks (CNN)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## License\n",
    "\n",
    "### Software and Content Description License\n",
    "Software code created by U.S. Government employees is not subject to copyright in the United States (17 U.S.C. ¬ß105). The United States/Department of Commerce reserve all rights to seek and obtain copyright protection in countries other than the United States for Software authored in its entirety by the Department of Commerce. To this end, the Department of Commerce hereby grants to Recipient a royalty-free, nonexclusive license to use, copy, and create derivative works of the Software outside of the United States."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Disclaimer\n",
    "\n",
    "> This Jupyter notebook is a scientific product and is not official communication of the National Oceanic and Atmospheric Administration, or the United States Department of Commerce. All NOAA Jupyter notebooks are provided on an 'as is' basis and the user assumes responsibility for its use. Any claims against the Department of Commerce or Department of Commerce bureaus stemming from the use of this Jupyter notebook will be governed by all applicable Federal law. Any reference to specific commercial products, processes, or services by service mark, trademark, manufacturer, or otherwise does not constitute or imply their endorsement, recommendation or favoring by the Department of Commerce. The Department of Commerce seal and logo, or the seal and logo of a DOC bureau, shall not be used in any manner to imply endorsement of any commercial product or activity by DOC or the United States Government."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}